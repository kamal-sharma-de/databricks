from pyspark.sql.functions import col, avg, hour

taxi_trips = spark.read.format("delta").load("nyc_taxi_trips")

yellow_cabs = taxi_trips.filter(col("vendor_id") == 1)

#Exploratory data analysis with pandas
avg_fare_borough = yellow_cabs.groupBy("passenger_count", "borough").agg(avg("fare_amount")).toPandas()

#Visualization 
busiest_hour = yellow_cabs.groupBy(hour(col("pickup_datetime"))).count().orderBy("count", descending=True).toPandas()

# yellow_cabs.write.format("delta").save("cleaned_taxi_trips")

# Display results
display(avg_fare_borough)
print(busiest_hour.head())
